
邹月明 20131101-05:05
======================
1. libgrocket
   已经把 tcp_req 的分配与释放简单的优化了一下。
2. 将性能测试客户端的统计错误改了一下，
   现在能达到48万请求每秒，20个线程，客户端也在本机
             46万请求每秒，10个线程，客户端也在本机
5. 私有 revision 136 对应 grocket revision 13
6. 修改了在Mac和Windows上的编译错。
7. 私有 revision 139 对应 grocket revision 14

邹月明 20131031-06:12
======================
1. libgrocket
   第一步准备优化tcp_req的分配与释放，在代码中已经在分配和释放tcp_req的地方以参数的形式将线程指针传过来了。

   10个服务器线程, 20个客户端连接的统计结果似乎让人怀疑是不是统计算法错了? 76万每秒?
   [AVG: 760 REQ/MS][unit=2000000][count=44][time_sum=115792]

邹月明 20131030-05:55
======================
1. libgrocket
   设置了一下CPU亲缘性，感觉连接性能将比昨天又有大幅提升，超过40W应该是没问题的。

2. 19:30
   早上的调整用处不大，结果发现是CS和SY值过大，又试着改了另一个地方：
   optimize single connection performance
   add try_to_send_tcp_rsp function，将发返回包的尝试放在处理线程直接做，发不出去了
   再用EPOLLOUT，这回有明显的性能提升了！峰值54万每秒
	thread  0: [ 54 REQ/MS][kiss=100001][unit=100000][send=4400044][recv=4400044][use_time=1821][avg:1821/1=1821]
	thread  6: [ 43 REQ/MS][kiss=100001][unit=100000][send=4400044][recv=4400044][use_time=2287][avg:2287/1=2287]
	thread  7: [ 43 REQ/MS][kiss=100001][unit=100000][send=4400044][recv=4400044][use_time=2299][avg:2299/1=2299]
	thread  1: [ 43 REQ/MS][kiss=100001][unit=100000][send=4400044][recv=4400044][use_time=2300][avg:2300/1=2300]
	thread  4: [ 42 REQ/MS][kiss=100001][unit=100000][send=4400044][recv=4400044][use_time=2340][avg:2340/1=2340]

    24核的机器，服务器开了20个线程，客户端20个线程。是的，CPU会有竟争，cs会比较大。

	现在的主要问题是cs比较高，由于sy列也高，所以cs高的主要原因应该是系统调用的原因
	procs -----------memory---------- ---swap-- -----io---- --system-- ----cpu----
	 r  b   swpd   free   buff  cache   si   so    bi    bo   in    cs us sy id wa
	 0  0      0 1467912 428580 60358896    0    0     0     8 25177  4497  0  0 99  0
	28  0      0 1469044 428580 60358920    0    0     0     0 25582 290412  3 14 83  0
	29  0      0 1467808 428580 60358932    0    0     0     0 25228 1470195 13 72 14  0
	22  0      0 1472784 428580 60358936    0    0     0     0 25280 1468366 14 72 14  0

	但用 strace 看一眼，已经优化到实在没什么可优化的程度了，发包都没用EPOLLOUT
	epoll_wait(7, {{EPOLLIN, {u32=4034275136, u64=140363565508416}}}, 10000, 100) = 1
	recvfrom(42, "71351 \0AAAAAAAAAAAAAAAAAAAAAAAAA"..., 255, MSG_NOSIGNAL, NULL, NULL) = 44
	recvfrom(42, 0x7fa8f0644b2c, 211, 16384, 0, 0) = -1 EAGAIN (Resource temporarily unavailable)
	sendto(42, "71351 \0AAAAAAAAAAAAAAAAAAAAAAAAA"..., 44, MSG_NOSIGNAL, NULL, 0) = 44
	epoll_wait(7, {{EPOLLIN, {u32=4034275136, u64=140363565508416}}}, 10000, 100) = 1
	recvfrom(42, "71352 \0AAAAAAAAAAAAAAAAAAAAAAAAA"..., 255, MSG_NOSIGNAL, NULL, NULL) = 44
	recvfrom(42, 0x7fa8f0644b2c, 211, 16384, 0, 0) = -1 EAGAIN (Resource temporarily unavailable)
	sendto(42, "71352 \0AAAAAAAAAAAAAAAAAAAAAAAAA"..., 44, MSG_NOSIGNAL, NULL, 0) = 44
	epoll_wait(7, {{EPOLLIN, {u32=4034275136, u64=140363565508416}}}, 10000, 100) = 1
	recvfrom(42, "71353 \0AAAAAAAAAAAAAAAAAAAAAAAAA"..., 255, MSG_NOSIGNAL, NULL, NULL) = 44
	recvfrom(42, 0x7fa8f0644b2c, 211, 16384, 0, 0) = -1 EAGAIN (Resource temporarily unavailable)
	sendto(42, "71353 \0AAAAAAAAAAAAAAAAAAAAAAAAA"..., 44, MSG_NOSIGNAL, NULL, 0) = 44

	所以基本判断，目前的sy值是正常的，cs还要用不超CPU核数的线程（客户端＋服务器线程数不超）再跑一下。

邹月明 20131029-11:55
======================
1. libgrocket
   为了避免 epoll 可能产生的惊群行为，在tcp_in和tcp_out中为每个线程创建一个epoll句柄。
   似乎性能又有一点儿提升，因为在epoll句柄中增删SOCKET过程中的并发少了。
2. 增加 compiler_switch.h 头文件，把一些编译选项拿到一个统计的地方了，在需要服务器测试
   性能时可以在一个地方关掉所有影响性能的开关。
3. 日志输出 errno 或 GetLastError()
4. 简单跑了一下，tcp.in.thread_count = 10，test_tcp_client 用 10 个连接。一秒钟可以跑 40 万请求。
   这说明单连接的处理能力基本已经及格了，再剩下的事情就可以开始尝试调试大并发的性能了。
    thread 4: 400001 kiss package, send 17600044 bytes, recv 17600044 bytes. 100000 package use 2412 ms, avg: 9809/4=2452 ms, 40 REQ/MS
    thread 0: 400001 kiss package, send 17600044 bytes, recv 17600044 bytes. 100000 package use 2336 ms, avg: 9816/4=2454 ms, 40 REQ/MS
    thread 3: 400001 kiss package, send 17600044 bytes, recv 17600044 bytes. 100000 package use 2564 ms, avg: 9952/4=2488 ms, 40 REQ/MS
    thread 2: 400001 kiss package, send 17600044 bytes, recv 17600044 bytes. 100000 package use 2517 ms, avg: 10040/4=2510 ms, 39 REQ/MS
    thread 8: 400001 kiss package, send 17600044 bytes, recv 17600044 bytes. 100000 package use 2498 ms, avg: 10042/4=2510 ms, 39 REQ/MS
    thread 1: 400001 kiss package, send 17600044 bytes, recv 17600044 bytes. 100000 package use 2410 ms, avg: 10044/4=2511 ms, 39 REQ/MS
    thread 6: 400001 kiss package, send 17600044 bytes, recv 17600044 bytes. 100000 package use 2591 ms, avg: 10099/4=2524 ms, 39 REQ/MS
    thread 5: 400001 kiss package, send 17600044 bytes, recv 17600044 bytes. 100000 package use 2432 ms, avg: 10210/4=2552 ms, 39 REQ/MS
    thread 7: 400001 kiss package, send 17600044 bytes, recv 17600044 bytes. 100000 package use 2554 ms, avg: 10219/4=2554 ms, 39 REQ/MS
    thread 9: 400001 kiss package, send 17600044 bytes, recv 17600044 bytes. 100000 package use 2558 ms, avg: 10237/4=2559 ms, 39 REQ/MS
5. 私有 revision 120 对应 grocket revision 12

邹月明 20131027-10:32
======================
1. libgrocket
   在配置文件里增加了 tcp_out.disabled 配置项，默认为 true。此值为 true  时性能提升明显。

   1) 正常情况下，应该将 worker.disabled 设为 true，这会达到服务器的最高性能；
   2) 但如果下行数据要做流控，如多媒体数据等，将  worker.disabled 设为
      false 让发送在单独的线程，不会让发送的流控阻塞数据包接收和处理。
2. 17:51
   私有 revision 106 对应 grocket revision 11。

邹月明 20131026-07:53
======================
1. libgrocket
   在配置文件里增加了 worker.disabled 配置项，默认为 true。此值为 true  时性能提升明显。

   1) 如果处理数据包比请求来包速度快而且处理速度平均，应该将 worker.disabled
      设为 true，这会达到服务器的最高性能；  
   2) 但如果数据包处理速度慢或者处理速度不平均，将  worker.disabled 设为  
      false 比较合适。该场景下，在处理速度比请求来包速度稍微慢一丁点儿点儿，  
      而有极少量积压请求时，性能最高。  
2. 16:23
   正在增加功能允许将 tcp out 线程也去掉。目前功能还没通。
3. 20:04
   在 Mac OS X 10.9 + XCode 5.01 开发环境下又重新可以编译成功了。苹果每升级一次XCode，
   程序都TMD要编译错，真NM不知苹果这SB是怎么想的。
   妈了个粪的能连接成功运行失败！难道只能用XCode编译了？
4. 23:45
   用 XCode 建了个工程，编译出来的二进制文件还是不能工作，怀疑 tcmalloc 2.1 在 xcode 5.01
   上已经不能工作了。
   
邹月明 20131025-06:34
======================
1. libgrocket
    将 is_server_stopping 等一些字段从 gr_global_t 中移到了 gr_server_t 中给调用方暴露
    调整了 grocket.h 文件，服务器的对外接口更合理了。
    服务器框架为模块为每个TCP连接保存了一个指针。

邹月明 20131024-00:45
======================
1. libgrocket
    增加了 http 协议解析。但现在还只是做准备，并不能工作。
    HTTP的工作做到这儿，就准备暂停一下，先应该确认一下线程和I/O模型的工作状态是否正常。

    后来一想不对，要和nginx对比，就要把 HTTP协议支持完成，于是乎把HTTP请求处理支持了。

邹月明 20131023-00:05
======================
1. libgrocket
    给模块增加了一个 gr_version 函数进行版本兼容性检查。
    现在已经不需要用户做这个工作了，由框架进行强制性检查。

    grocket revision 10

    单连接TCP已经通了！

邹月明 20131022-00:15
======================
1. libgrocket
   终于改好了TCP断连接会core的BUG。这可是 lock free、而且是将散布在多个的线程的同一个连接删除哦...
   在你急着怀疑设计上有没有问题之前，我先提醒一下，这个服务器框架当前阶段的TCP是偏向于长连接的，
   短连接性能相关的考虑会在将来以一个配置选项的方式来支持。

2. 还差一个触发断言的BUG：
#3  0x0000000000432ef9 in gr_tcp_conn_pop_top_req (conn=0x2aaaab2f2040) at libgrocket/gr_conn.c:336
336         assert( conn->req_push_count >= conn->req_proc_count );

   原因如下：
[root@localhost server]# ./bin/linux/test_tcp_client 
 856 kiss package, send 37664 bytes, recv 37664 bytes. req=856 ;rsp=856 
compare failed: 857  != 856 

   请求包发的是857, 但回复包是856。
   可能的问题需要排除两个地方：
   1、tcp in 收包以后向 worker 中压包过程的问题；
   2、worker 处理以后向 conn->rsp_list_* 中压包过程的问题。这个最可能出问题，因为它还没调过。但改了似乎还是一样。
   似乎是 gr_poll_send 函数操作 conn->rsp_list_* 字段的BUG，这部分目前没按照无锁队列的规则访问。

邹月明 20131020-19:24
======================
1. libgrocket
   差不多完成了断连接的逻辑，虽然可能会有一些隐含的问题，但可以留在后面做压力测试时一起解决。

邹月明 20131019-22:18
======================
1. test_tcp_client
   增加了 TCP 测试客户端程序

邹月明 20131017-00:25
======================
1. libgrocket
   在 gr_tcp_close.c 源文件中, 对 TCP 断连接的流程做了一些思考

邹月明 20131010-01:00
======================
1. libgrocket
   丰富 server_object.c 的功能，在demo_module.c 里增加了 echo 服务器的功能。

邹月明 20131009-01:05
======================
1. libgrocket

   到目前为止，在 Windows、Linux、Mac 上都可以收发包了。

   grocket.h 里增加了 gr_library_t、gr_class_t、gr_object_t，这是服务器函数库的接口暴露形式。
   增加了 gr_library_invoke.h 实现了通用接口函数调用的实现。

   还有一个问题要解决：因为服务器对外暴露的接口只有头文件，没有库，如何让一个对象声明和一个
   二进制对象指针安全的建立起关系？初步解决了，见server_object.c文件中的check函数。

   !!!!!!!!!!!!!!!
   在 demo_module.c 里找 o->config_get_int 可以看到服务器框架提供的一处很有特色的功能。
   该类在服务器框架中的实现就在server_object.c里。再看一眼gr_library_invoke.c，我想你能猜出我
   要如何为用户暴露第三方函数库的大致思路了，是不是很赞？

   截止到目前，源代码行数是14767行，平均一天900多行跨平台代码。

邹月明 20131007-22:21
======================
1. libgrocket
   今天到目前为止干了17个小时，稍带脚把 BSD 和 Mac OS X 系统也支持了。现在程序可以
   跨的平台有：
   Windows 32／64、Linux 32／64，BSD／Mac OS X

   Windows不允许把一个SOCKET加到多个IOCP里，这是一个架构冲击。Windows操作系统事后
   通知的IO模型和其它操作系统的事前通知的IO模型本来就区别极大，再加上这个限制，真TMD
   让人想骂娘。

   修改实现让 tcp_in 和 tcp_out共用一个IOCP。不得不把本应放在.c文件中的一部分内容
   提出来到 tcp_io.h 文件中，该文件有 gr_tcp_in.c、gr_tcp_out.c 和 gr_poll_windows.c
   依赖。本次修改的原则和策略有以下几点：
   1) 对非Windows的架构、性能影响为0
   2) 万一以后Windows支持把SOCKET加到多个IOCP里，只需把增加的代码删除即可恢复优美的
      程序架构。（估计Windows支持该功能的可能性为0）
   3) 对数据收、发的业务处理逻辑，所有平台都必须只使用一份，便于维护。这也是为什么
      有tcp_io.h文件的原因。
   4) 由于收、发合并，导致理想中的收、发分别处理的逻辑在Windows上行不通了，必须要想办法
      判断当前是收动作还是发动作触发的事件，然后根据情况不同处理，所以增加了一个
      tcp_io_windows 函数给Windows用，其它平台还是保持理想的代码架构不变。

邹月明 20131005-23:15
======================
1. libgrocket
   正常TCP数据处理流程已经到tcp_out了，tcp_out还没实现

邹月明 20131004-05:26
======================
1. libgrocket
   将eventfd加入gr_event

邹月明 20131003-18:01
======================
1. libgrocket
   将network_in拆成tcp_accept, tcp_in, udp_in三部分, 这可以
   让 tcp_accept 的逻辑尽量的少, 它就可以有空闲时间用来删连接了
   将network_out拆成tcp_out, udp_out两部分.
   
   现在绝大部分代码的文件名从名称看起来都更直接了

邹月明 20131001-22:16
======================
1. libgrocket
   增加了gr_lock.h 线程锁
   增加了gr_queue.h, gr_queue.cpp 一个单线程写、单线程读的队列，尽量高性能
   增加了gr_list.h 一个双链表

邹月明 20130929-11:40
======================
1. demo_server
   完成了 demo 如何基于本框架写一个独立的服务器
2. libgrocketd
   增加了 gr_poll_t 用于封装 Windows 的 IOCP 与 Linux的 EPOLL 的差别。
   差别如此之大的IO模型做成这种统一还相对简单的接口，我目前还算一般满意。
   只是当初希望它是一个非常独立的类的想法没完成，它还是对其它的一些类有
   依赖，这也可以理解，因为这个东西本身就是架构侵入而且差别非常大的。

邹月明 20130928-01:25
======================
1. grocketd, libgrocket, demo_module
   完成了示范服务器端模块

邹月明 20130927-01:15
======================
1. libgrocketd
   内部模块隔离与访问方法彻底清晰了，增加了 gr_global.h 做为整个程序中
   唯一的全局变量。

邹月明 20130926-12:41
======================
1. libgrocketd
   继续完善内部模块划分，增加了几个工具类，内存分配使用tcmalloc 2.1

邹月明 20130925-02:37
======================
1. grocketd, libgrocketd

   本项目从今天开始

   完成了服务器框架对外接口的外部模块划分和内部模块划分，
   完成了服务器框架对外接口文件 gsocket.h
   接口支持以独立进程形式和调用方提供进程形式。
   先从同时支持32位和64位Windows和Linux开始。
   用C写，不是不会C++，是为了追求最佳性能和最简单的设计。
   比如红黑树等复杂算法，在用C写这种性能相关的程序时原则上基本不会考虑使用，
   尽量使用数组一类的最简单最直接的算法，必要时考虑空间换时间)。

   目标：一个HTTP性能超越 nginx 的服务器框架
   详见 ../../docs/idea.txt
